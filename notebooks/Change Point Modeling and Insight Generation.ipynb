{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0e8a21",
   "metadata": {},
   "source": [
    "## Core Analysis Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5686f",
   "metadata": {},
   "source": [
    "### 1.1 Data Preparation and EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d00276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('BrentOilPrices.csv', parse_dates=['Date'])\n",
    "df = df.sort_values('Date')\n",
    "df = df.set_index('Date')\n",
    "\n",
    "# Focus on recent decade for clearer event association\n",
    "start_date = '2012-01-01'\n",
    "recent_df = df.loc[start_date:].copy()\n",
    "\n",
    "# Plot raw price series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Raw price series\n",
    "axes[0, 0].plot(recent_df.index, recent_df['Price'])\n",
    "axes[0, 0].set_title('Brent Oil Prices (2012-2022)')\n",
    "axes[0, 0].set_ylabel('USD per Barrel')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 2. Log returns\n",
    "recent_df['Log_Return'] = np.log(recent_df['Price']) - np.log(recent_df['Price'].shift(1))\n",
    "axes[0, 1].plot(recent_df.index, recent_df['Log_Return'])\n",
    "axes[0, 1].set_title('Daily Log Returns')\n",
    "axes[0, 1].set_ylabel('Log Return')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 3. Histogram of returns\n",
    "axes[1, 0].hist(recent_df['Log_Return'].dropna(), bins=50, edgecolor='black')\n",
    "axes[1, 0].set_title('Distribution of Log Returns')\n",
    "axes[1, 0].set_xlabel('Log Return')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Rolling volatility (30-day)\n",
    "axes[1, 1].plot(recent_df.index, recent_df['Log_Return'].rolling(30).std())\n",
    "axes[1, 1].set_title('30-Day Rolling Volatility')\n",
    "axes[1, 1].set_ylabel('Volatility')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for modeling\n",
    "price_data = recent_df['Price'].values\n",
    "n = len(price_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d7953",
   "metadata": {},
   "source": [
    "## Build Bayesian Change Point Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as change_point_model:\n",
    "    \n",
    "    # Prior for change point location (uniform over all days)\n",
    "    tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n-1)\n",
    "    \n",
    "    # Priors for means before and after change point\n",
    "    mu1 = pm.Normal(\"mu1\", mu=np.mean(price_data), sigma=10)\n",
    "    mu2 = pm.Normal(\"mu2\", mu=np.mean(price_data), sigma=10)\n",
    "    \n",
    "    # Standard deviation (could also make this change)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n",
    "    \n",
    "    # Switch function: use mu1 before tau, mu2 after tau\n",
    "    mean = pm.math.switch(tau > np.arange(n), mu1, mu2)\n",
    "    \n",
    "    # Likelihood - normal distribution\n",
    "    likelihood = pm.Normal(\"likelihood\", mu=mean, sigma=sigma, observed=price_data)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    trace = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=1000,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        return_inferencedata=True,\n",
    "        random_seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda9d9a4",
   "metadata": {},
   "source": [
    "## Interpret Model Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68229ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence diagnostics\n",
    "print(\"Convergence Diagnostics:\")\n",
    "print(\"=\" * 50)\n",
    "summary = az.summary(trace)\n",
    "print(summary[['mean', 'sd', 'hdi_3%', 'hdi_97%', 'r_hat']])\n",
    "\n",
    "# R-hat values close to 1.0 indicate good convergence\n",
    "if all(summary['r_hat'] < 1.05):\n",
    "    print(\"\\n✓ All R-hat values < 1.05 - Good convergence achieved\")\n",
    "else:\n",
    "    print(\"\\n⚠ Some R-hat values > 1.05 - Consider more samples\")\n",
    "\n",
    "# Trace plots\n",
    "az.plot_trace(trace)\n",
    "plt.suptitle('Trace Plots for Model Parameters', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('trace_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Posterior distribution of tau\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 1. Histogram of tau posterior\n",
    "tau_samples = trace.posterior['tau'].values.flatten()\n",
    "axes[0].hist(tau_samples, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(tau_samples), color='red', linestyle='--', label=f'Mean: {int(np.mean(tau_samples))}')\n",
    "axes[0].set_xlabel('τ (index)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Posterior Distribution of Change Point (τ)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 2. Convert to actual dates\n",
    "change_point_idx = int(np.mean(tau_samples))\n",
    "change_point_date = recent_df.index[change_point_idx]\n",
    "axes[1].plot(recent_df.index, recent_df['Price'], alpha=0.7)\n",
    "axes[1].axvline(change_point_date, color='red', linestyle='--', \n",
    "                label=f'Change Point: {change_point_date.strftime(\"%Y-%m-%d\")}')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Price (USD)')\n",
    "axes[1].set_title('Price Series with Detected Change Point')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('change_point_detection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDetected Change Point: {change_point_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Mean price before: ${trace.posterior['mu1'].mean().values:.2f}\")\n",
    "print(f\"Mean price after: ${trace.posterior['mu2'].mean().values:.2f}\")\n",
    "print(f\"Percentage change: {((trace.posterior['mu2'].mean().values - trace.posterior['mu1'].mean().values) / trace.posterior['mu1'].mean().values * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36b121",
   "metadata": {},
   "source": [
    "## Associate Changes with Causes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events data\n",
    "events_df = pd.read_csv('historical_events.csv', parse_dates=['event_date'])\n",
    "\n",
    "# Find closest event to detected change point\n",
    "def find_closest_event(change_date, events_df, window_days=30):\n",
    "    \"\"\"Find events within a window of the change point\"\"\"\n",
    "    closest_events = []\n",
    "    for _, event in events_df.iterrows():\n",
    "        days_diff = abs((change_date - event['event_date']).days)\n",
    "        if days_diff <= window_days:\n",
    "            closest_events.append({\n",
    "                'event_name': event['event_name'],\n",
    "                'event_date': event['event_date'].strftime('%Y-%m-%d'),\n",
    "                'days_diff': days_diff,\n",
    "                'event_type': event['event_type'],\n",
    "                'expected_impact': event['expected_impact']\n",
    "            })\n",
    "    \n",
    "    # Sort by proximity\n",
    "    closest_events.sort(key=lambda x: x['days_diff'])\n",
    "    return closest_events[:3]  # Return top 3 closest events\n",
    "\n",
    "closest_events = find_closest_event(change_point_date, events_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVENT ASSOCIATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Detected change point: {change_point_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"\\nClosest historical events (±30 days):\")\n",
    "\n",
    "for i, event in enumerate(closest_events, 1):\n",
    "    print(f\"\\n{i}. {event['event_name']}\")\n",
    "    print(f\"   Event Date: {event['event_date']}\")\n",
    "    print(f\"   Days from change point: {event['days_diff']}\")\n",
    "    print(f\"   Event Type: {event['event_type']}\")\n",
    "    print(f\"   Expected Impact: {event['expected_impact']}\")\n",
    "\n",
    "# Quantify impact\n",
    "mu1_mean = trace.posterior['mu1'].mean().values\n",
    "mu2_mean = trace.posterior['mu2'].mean().values\n",
    "price_change = mu2_mean - mu1_mean\n",
    "percent_change = (price_change / mu1_mean) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"QUANTIFIED IMPACT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean price before change point: ${mu1_mean:.2f}\")\n",
    "print(f\"Mean price after change point: ${mu2_mean:.2f}\")\n",
    "print(f\"Absolute change: ${price_change:.2f}\")\n",
    "print(f\"Percentage change: {percent_change:.1f}%\")\n",
    "\n",
    "# Generate impact statement\n",
    "if closest_events:\n",
    "    closest_event = closest_events[0]\n",
    "    impact_statement = (\n",
    "        f\"Following {closest_event['event_name']} around {closest_event['event_date']}, \"\n",
    "        f\"the model detects a structural break in Brent oil prices on {change_point_date.strftime('%Y-%m-%d')}. \"\n",
    "        f\"The average daily price shifted from ${mu1_mean:.2f} to ${mu2_mean:.2f}, \"\n",
    "        f\"representing a {percent_change:.1f}% {'increase' if percent_change > 0 else 'decrease'}.\"\n",
    "    )\n",
    "    print(f\"\\nIMPACT STATEMENT:\\n{impact_statement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb10234",
   "metadata": {},
   "source": [
    "##  Incorporating Additional Factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework for multivariate analysis\n",
    "def build_comprehensive_model(price_data, gdp_data=None, inflation_data=None):\n",
    "    \"\"\"Extended model with macroeconomic factors\"\"\"\n",
    "    \n",
    "    with pm.Model() as extended_model:\n",
    "        \n",
    "        # Multiple change points (extending to multiple breaks)\n",
    "        n_changepoints = 3\n",
    "        taus = []\n",
    "        for i in range(n_changepoints):\n",
    "            tau = pm.DiscreteUniform(f\"tau_{i}\", \n",
    "                                    lower=0 if i == 0 else taus[i-1]+1, \n",
    "                                    upper=len(price_data)-1)\n",
    "            taus.append(tau)\n",
    "        \n",
    "        # Sort change points\n",
    "        taus_sorted = pm.math.sort(pm.math.stack(taus))\n",
    "        \n",
    "        # Segment means\n",
    "        mus = []\n",
    "        for i in range(n_changepoints + 1):\n",
    "            mu = pm.Normal(f\"mu_{i}\", mu=np.mean(price_data), sigma=20)\n",
    "            mus.append(mu)\n",
    "        \n",
    "        # Create mean array based on change points\n",
    "        mean_array = mus[0] * np.ones(len(price_data))\n",
    "        for i, tau in enumerate(taus_sorted):\n",
    "            mask = np.arange(len(price_data)) >= tau\n",
    "            if i < len(mus) - 1:\n",
    "                mean_array = pm.math.switch(mask, mus[i+1], mean_array)\n",
    "        \n",
    "        # Volatility clustering (GARCH-like component)\n",
    "        sigma = pm.HalfNormal(\"sigma\", sigma=5)\n",
    "        \n",
    "        # Likelihood\n",
    "        likelihood = pm.Normal(\"likelihood\", mu=mean_array, sigma=sigma, observed=price_data)\n",
    "    \n",
    "    return extended_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d55cc0",
   "metadata": {},
   "source": [
    "##  Model Comparison Framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models_dict, price_data):\n",
    "    \"\"\"Compare different change point models\"\"\"\n",
    "    \n",
    "    model_traces = {}\n",
    "    model_metrics = {}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nFitting {model_name}...\")\n",
    "        \n",
    "        with model:\n",
    "            trace = pm.sample(draws=1000, tune=500, chains=2, random_seed=42)\n",
    "            model_traces[model_name] = trace\n",
    "            \n",
    "            # Calculate WAIC for model comparison\n",
    "            waic = az.waic(trace)\n",
    "            model_metrics[model_name] = {\n",
    "                'WAIC': waic.waic,\n",
    "                'pWAIC': waic.p_waic,\n",
    "                'WAIC_se': waic.waic_se\n",
    "            }\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame(model_metrics).T\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Select best model (lowest WAIC)\n",
    "    best_model = comparison_df['WAIC'].idxmin()\n",
    "    print(f\"\\n✓ Best model: {best_model} (lowest WAIC)\")\n",
    "    \n",
    "    return model_traces, comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
